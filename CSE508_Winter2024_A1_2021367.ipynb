{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afyBX-Ku3Rqo",
        "outputId": "b9da1d1d-561f-4eee-f57b-fb9c3eda414b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the text preprocessing function\n",
        "def preprocess_text(text, print_content=False):\n",
        "    # Print original text if required\n",
        "    if print_content:\n",
        "        print(\"Original Text:\\n\", text)\n",
        "\n",
        "    # Parse HTML content if any, using BeautifulSoup\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Lowercase the text\n",
        "    text_lower = text.lower()\n",
        "    if print_content:\n",
        "        print(\"\\nAfter Lowercasing:\\n\", text_lower)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text_lower)\n",
        "    if print_content:\n",
        "        print(\"\\nAfter Tokenization:\\n\", tokens)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_no_stopwords = [word for word in tokens if word not in stop_words]\n",
        "    if print_content:\n",
        "        print(\"\\nAfter Removing Stopwords:\\n\", tokens_no_stopwords)\n",
        "\n",
        "    # Remove punctuation and keep only alphabetic tokens\n",
        "    tokens_no_punctuation = [word for word in tokens_no_stopwords if word.isalpha()]\n",
        "    if print_content:\n",
        "        print(\"\\nAfter Removing Punctuation:\\n\", tokens_no_punctuation)\n",
        "\n",
        "    # Join tokens back to a single string\n",
        "    preprocessed_text = ' '.join(tokens_no_punctuation)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "# Function to preprocess files within a specified directory\n",
        "def preprocess_files(directory_path):\n",
        "    # Create a folder to store preprocessed files if it doesn't exist\n",
        "    preprocessed_dir = os.path.join(directory_path, 'preprocessed_files')\n",
        "    os.makedirs(preprocessed_dir, exist_ok=True)\n",
        "\n",
        "    # Get a list of file paths excluding directories\n",
        "    file_paths = [os.path.join(directory_path, file_name) for file_name in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, file_name))]\n",
        "\n",
        "    # Randomly select 5 files to display contents before and after preprocessing\n",
        "    sample_files = random.sample(file_paths, min(5, len(file_paths)))\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # Read file content\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Check if the file is one of the sample files to print content\n",
        "        print_content = file_path in sample_files\n",
        "\n",
        "        # Preprocess text\n",
        "        preprocessed_text = preprocess_text(text, print_content)\n",
        "\n",
        "        # Save the preprocessed text to a new file in the preprocessed_files directory\n",
        "        file_name = os.path.basename(file_path)\n",
        "        preprocessed_file_path = os.path.join(preprocessed_dir, file_name.replace('.txt', '_preprocessed.txt'))\n",
        "        with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(preprocessed_text)\n",
        "\n",
        "        if print_content:\n",
        "            print(f\"\\nPreprocessed text saved: {preprocessed_file_path}\\n\")\n",
        "            print('//////////////////////////////////////////////\\n')\n",
        "\n",
        "# Usage example with a Google Drive path\n",
        "directory_path = '/content/drive/My Drive/IR_Assignment1/text_files'  # Adjust this path as needed\n",
        "preprocess_files(directory_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2ruFG6P3nh4",
        "outputId": "d06abb01-cb60-45c2-afa2-a2d72e8e7eae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-12-2aa4343858c8>:25: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " Love the blue glow of the power supply on my pedal board.  I've only had it for a day so I don't know how it will hold up, but i'm sure it will be fine.  Comes with all of the cables to fully power your pedal board.  The A/C adapter cable is only 3 feet long so consider buying an extension chord or power strip to plug it into.\n",
            "\n",
            "After Lowercasing:\n",
            " love the blue glow of the power supply on my pedal board.  i've only had it for a day so i don't know how it will hold up, but i'm sure it will be fine.  comes with all of the cables to fully power your pedal board.  the a/c adapter cable is only 3 feet long so consider buying an extension chord or power strip to plug it into.\n",
            "\n",
            "After Tokenization:\n",
            " ['love', 'the', 'blue', 'glow', 'of', 'the', 'power', 'supply', 'on', 'my', 'pedal', 'board', '.', 'i', \"'ve\", 'only', 'had', 'it', 'for', 'a', 'day', 'so', 'i', 'do', \"n't\", 'know', 'how', 'it', 'will', 'hold', 'up', ',', 'but', 'i', \"'m\", 'sure', 'it', 'will', 'be', 'fine', '.', 'comes', 'with', 'all', 'of', 'the', 'cables', 'to', 'fully', 'power', 'your', 'pedal', 'board', '.', 'the', 'a/c', 'adapter', 'cable', 'is', 'only', '3', 'feet', 'long', 'so', 'consider', 'buying', 'an', 'extension', 'chord', 'or', 'power', 'strip', 'to', 'plug', 'it', 'into', '.']\n",
            "\n",
            "After Removing Stopwords:\n",
            " ['love', 'blue', 'glow', 'power', 'supply', 'pedal', 'board', '.', \"'ve\", 'day', \"n't\", 'know', 'hold', ',', \"'m\", 'sure', 'fine', '.', 'comes', 'cables', 'fully', 'power', 'pedal', 'board', '.', 'a/c', 'adapter', 'cable', '3', 'feet', 'long', 'consider', 'buying', 'extension', 'chord', 'power', 'strip', 'plug', '.']\n",
            "\n",
            "After Removing Punctuation:\n",
            " ['love', 'blue', 'glow', 'power', 'supply', 'pedal', 'board', 'day', 'know', 'hold', 'sure', 'fine', 'comes', 'cables', 'fully', 'power', 'pedal', 'board', 'adapter', 'cable', 'feet', 'long', 'consider', 'buying', 'extension', 'chord', 'power', 'strip', 'plug']\n",
            "\n",
            "Preprocessed text saved: /content/drive/My Drive/IR_Assignment1/text_files/preprocessed_files/file223_preprocessed.txt\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Original Text:\n",
            " Love it! I have had other tuners & recently bought a pedal tuner. I like this because it's much more convenient. The pedal wears through 9 volt batteries pretty quick, so I'm constantly plugging & unplugging. W/ this one, I just move it from one headstock to another & turn it on/off as desired...\n",
            "\n",
            "note: using it for at home/practice/recording...\n",
            "\n",
            "After Lowercasing:\n",
            " love it! i have had other tuners & recently bought a pedal tuner. i like this because it's much more convenient. the pedal wears through 9 volt batteries pretty quick, so i'm constantly plugging & unplugging. w/ this one, i just move it from one headstock to another & turn it on/off as desired...\n",
            "\n",
            "note: using it for at home/practice/recording...\n",
            "\n",
            "After Tokenization:\n",
            " ['love', 'it', '!', 'i', 'have', 'had', 'other', 'tuners', '&', 'recently', 'bought', 'a', 'pedal', 'tuner', '.', 'i', 'like', 'this', 'because', 'it', \"'s\", 'much', 'more', 'convenient', '.', 'the', 'pedal', 'wears', 'through', '9', 'volt', 'batteries', 'pretty', 'quick', ',', 'so', 'i', \"'m\", 'constantly', 'plugging', '&', 'unplugging', '.', 'w/', 'this', 'one', ',', 'i', 'just', 'move', 'it', 'from', 'one', 'headstock', 'to', 'another', '&', 'turn', 'it', 'on/off', 'as', 'desired', '...', 'note', ':', 'using', 'it', 'for', 'at', 'home/practice/recording', '...']\n",
            "\n",
            "After Removing Stopwords:\n",
            " ['love', '!', 'tuners', '&', 'recently', 'bought', 'pedal', 'tuner', '.', 'like', \"'s\", 'much', 'convenient', '.', 'pedal', 'wears', '9', 'volt', 'batteries', 'pretty', 'quick', ',', \"'m\", 'constantly', 'plugging', '&', 'unplugging', '.', 'w/', 'one', ',', 'move', 'one', 'headstock', 'another', '&', 'turn', 'on/off', 'desired', '...', 'note', ':', 'using', 'home/practice/recording', '...']\n",
            "\n",
            "After Removing Punctuation:\n",
            " ['love', 'tuners', 'recently', 'bought', 'pedal', 'tuner', 'like', 'much', 'convenient', 'pedal', 'wears', 'volt', 'batteries', 'pretty', 'quick', 'constantly', 'plugging', 'unplugging', 'one', 'move', 'one', 'headstock', 'another', 'turn', 'desired', 'note', 'using']\n",
            "\n",
            "Preprocessed text saved: /content/drive/My Drive/IR_Assignment1/text_files/preprocessed_files/file533_preprocessed.txt\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Original Text:\n",
            " I like these microphpone windscreens and the price can't be beat. I have a few lavalier microphones and wanted to use them for recording outdoors. Unfortunately they are larger than what I would consider a lav mic windscreen. I have found they work best with my olympus me51s lav mic and they significantly reduced wind/ambient noise outside. The colors are nice and help in location them and not misplacing them. I have lost so many small black lav wind screens that if I had a dime...well you know the rest. I included a picture in the customer gallery so you can see beyond the stock photo what you're buying. I would recommend and just mention again, be aware of the size you're getting.\n",
            "\n",
            "After Lowercasing:\n",
            " i like these microphpone windscreens and the price can't be beat. i have a few lavalier microphones and wanted to use them for recording outdoors. unfortunately they are larger than what i would consider a lav mic windscreen. i have found they work best with my olympus me51s lav mic and they significantly reduced wind/ambient noise outside. the colors are nice and help in location them and not misplacing them. i have lost so many small black lav wind screens that if i had a dime...well you know the rest. i included a picture in the customer gallery so you can see beyond the stock photo what you're buying. i would recommend and just mention again, be aware of the size you're getting.\n",
            "\n",
            "After Tokenization:\n",
            " ['i', 'like', 'these', 'microphpone', 'windscreens', 'and', 'the', 'price', 'ca', \"n't\", 'be', 'beat', '.', 'i', 'have', 'a', 'few', 'lavalier', 'microphones', 'and', 'wanted', 'to', 'use', 'them', 'for', 'recording', 'outdoors', '.', 'unfortunately', 'they', 'are', 'larger', 'than', 'what', 'i', 'would', 'consider', 'a', 'lav', 'mic', 'windscreen', '.', 'i', 'have', 'found', 'they', 'work', 'best', 'with', 'my', 'olympus', 'me51s', 'lav', 'mic', 'and', 'they', 'significantly', 'reduced', 'wind/ambient', 'noise', 'outside', '.', 'the', 'colors', 'are', 'nice', 'and', 'help', 'in', 'location', 'them', 'and', 'not', 'misplacing', 'them', '.', 'i', 'have', 'lost', 'so', 'many', 'small', 'black', 'lav', 'wind', 'screens', 'that', 'if', 'i', 'had', 'a', 'dime', '...', 'well', 'you', 'know', 'the', 'rest', '.', 'i', 'included', 'a', 'picture', 'in', 'the', 'customer', 'gallery', 'so', 'you', 'can', 'see', 'beyond', 'the', 'stock', 'photo', 'what', 'you', \"'re\", 'buying', '.', 'i', 'would', 'recommend', 'and', 'just', 'mention', 'again', ',', 'be', 'aware', 'of', 'the', 'size', 'you', \"'re\", 'getting', '.']\n",
            "\n",
            "After Removing Stopwords:\n",
            " ['like', 'microphpone', 'windscreens', 'price', 'ca', \"n't\", 'beat', '.', 'lavalier', 'microphones', 'wanted', 'use', 'recording', 'outdoors', '.', 'unfortunately', 'larger', 'would', 'consider', 'lav', 'mic', 'windscreen', '.', 'found', 'work', 'best', 'olympus', 'me51s', 'lav', 'mic', 'significantly', 'reduced', 'wind/ambient', 'noise', 'outside', '.', 'colors', 'nice', 'help', 'location', 'misplacing', '.', 'lost', 'many', 'small', 'black', 'lav', 'wind', 'screens', 'dime', '...', 'well', 'know', 'rest', '.', 'included', 'picture', 'customer', 'gallery', 'see', 'beyond', 'stock', 'photo', \"'re\", 'buying', '.', 'would', 'recommend', 'mention', ',', 'aware', 'size', \"'re\", 'getting', '.']\n",
            "\n",
            "After Removing Punctuation:\n",
            " ['like', 'microphpone', 'windscreens', 'price', 'ca', 'beat', 'lavalier', 'microphones', 'wanted', 'use', 'recording', 'outdoors', 'unfortunately', 'larger', 'would', 'consider', 'lav', 'mic', 'windscreen', 'found', 'work', 'best', 'olympus', 'lav', 'mic', 'significantly', 'reduced', 'noise', 'outside', 'colors', 'nice', 'help', 'location', 'misplacing', 'lost', 'many', 'small', 'black', 'lav', 'wind', 'screens', 'dime', 'well', 'know', 'rest', 'included', 'picture', 'customer', 'gallery', 'see', 'beyond', 'stock', 'photo', 'buying', 'would', 'recommend', 'mention', 'aware', 'size', 'getting']\n",
            "\n",
            "Preprocessed text saved: /content/drive/My Drive/IR_Assignment1/text_files/preprocessed_files/file211_preprocessed.txt\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Original Text:\n",
            " I got this for my son for Christmas and he really loves it.  He also as a condenser mic that needs phantom power in order to work.  With this focusrite scarlett 2i2 Audio Interface that eliminates that requirement as it also doubles as phantom power.  The mic is very clear when using this recording device.  He uses the Magix Music Maker to record his music.  It is excellent and he has not had any issues with it at all.  I asked him if there was anything negative about it and he said all he can think of is how perfect it is.  You can adjuster mic volume, very clear, he has not heard any cut outs.  The only con is if you leave it plugged in to the USB when you are not using it your voice will sound static, but he said its an easy fix you just unplug the USB and plug it back in.  By the way he is using an old dell laptop and it works great.\n",
            "I am very happy with this purchase and recommend it.\n",
            "\n",
            "After Lowercasing:\n",
            " i got this for my son for christmas and he really loves it.  he also as a condenser mic that needs phantom power in order to work.  with this focusrite scarlett 2i2 audio interface that eliminates that requirement as it also doubles as phantom power.  the mic is very clear when using this recording device.  he uses the magix music maker to record his music.  it is excellent and he has not had any issues with it at all.  i asked him if there was anything negative about it and he said all he can think of is how perfect it is.  you can adjuster mic volume, very clear, he has not heard any cut outs.  the only con is if you leave it plugged in to the usb when you are not using it your voice will sound static, but he said its an easy fix you just unplug the usb and plug it back in.  by the way he is using an old dell laptop and it works great.\n",
            "i am very happy with this purchase and recommend it.\n",
            "\n",
            "After Tokenization:\n",
            " ['i', 'got', 'this', 'for', 'my', 'son', 'for', 'christmas', 'and', 'he', 'really', 'loves', 'it', '.', 'he', 'also', 'as', 'a', 'condenser', 'mic', 'that', 'needs', 'phantom', 'power', 'in', 'order', 'to', 'work', '.', 'with', 'this', 'focusrite', 'scarlett', '2i2', 'audio', 'interface', 'that', 'eliminates', 'that', 'requirement', 'as', 'it', 'also', 'doubles', 'as', 'phantom', 'power', '.', 'the', 'mic', 'is', 'very', 'clear', 'when', 'using', 'this', 'recording', 'device', '.', 'he', 'uses', 'the', 'magix', 'music', 'maker', 'to', 'record', 'his', 'music', '.', 'it', 'is', 'excellent', 'and', 'he', 'has', 'not', 'had', 'any', 'issues', 'with', 'it', 'at', 'all', '.', 'i', 'asked', 'him', 'if', 'there', 'was', 'anything', 'negative', 'about', 'it', 'and', 'he', 'said', 'all', 'he', 'can', 'think', 'of', 'is', 'how', 'perfect', 'it', 'is', '.', 'you', 'can', 'adjuster', 'mic', 'volume', ',', 'very', 'clear', ',', 'he', 'has', 'not', 'heard', 'any', 'cut', 'outs', '.', 'the', 'only', 'con', 'is', 'if', 'you', 'leave', 'it', 'plugged', 'in', 'to', 'the', 'usb', 'when', 'you', 'are', 'not', 'using', 'it', 'your', 'voice', 'will', 'sound', 'static', ',', 'but', 'he', 'said', 'its', 'an', 'easy', 'fix', 'you', 'just', 'unplug', 'the', 'usb', 'and', 'plug', 'it', 'back', 'in', '.', 'by', 'the', 'way', 'he', 'is', 'using', 'an', 'old', 'dell', 'laptop', 'and', 'it', 'works', 'great', '.', 'i', 'am', 'very', 'happy', 'with', 'this', 'purchase', 'and', 'recommend', 'it', '.']\n",
            "\n",
            "After Removing Stopwords:\n",
            " ['got', 'son', 'christmas', 'really', 'loves', '.', 'also', 'condenser', 'mic', 'needs', 'phantom', 'power', 'order', 'work', '.', 'focusrite', 'scarlett', '2i2', 'audio', 'interface', 'eliminates', 'requirement', 'also', 'doubles', 'phantom', 'power', '.', 'mic', 'clear', 'using', 'recording', 'device', '.', 'uses', 'magix', 'music', 'maker', 'record', 'music', '.', 'excellent', 'issues', '.', 'asked', 'anything', 'negative', 'said', 'think', 'perfect', '.', 'adjuster', 'mic', 'volume', ',', 'clear', ',', 'heard', 'cut', 'outs', '.', 'con', 'leave', 'plugged', 'usb', 'using', 'voice', 'sound', 'static', ',', 'said', 'easy', 'fix', 'unplug', 'usb', 'plug', 'back', '.', 'way', 'using', 'old', 'dell', 'laptop', 'works', 'great', '.', 'happy', 'purchase', 'recommend', '.']\n",
            "\n",
            "After Removing Punctuation:\n",
            " ['got', 'son', 'christmas', 'really', 'loves', 'also', 'condenser', 'mic', 'needs', 'phantom', 'power', 'order', 'work', 'focusrite', 'scarlett', 'audio', 'interface', 'eliminates', 'requirement', 'also', 'doubles', 'phantom', 'power', 'mic', 'clear', 'using', 'recording', 'device', 'uses', 'magix', 'music', 'maker', 'record', 'music', 'excellent', 'issues', 'asked', 'anything', 'negative', 'said', 'think', 'perfect', 'adjuster', 'mic', 'volume', 'clear', 'heard', 'cut', 'outs', 'con', 'leave', 'plugged', 'usb', 'using', 'voice', 'sound', 'static', 'said', 'easy', 'fix', 'unplug', 'usb', 'plug', 'back', 'way', 'using', 'old', 'dell', 'laptop', 'works', 'great', 'happy', 'purchase', 'recommend']\n",
            "\n",
            "Preprocessed text saved: /content/drive/My Drive/IR_Assignment1/text_files/preprocessed_files/file631_preprocessed.txt\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Original Text:\n",
            " Oh my word I am sold I love my AT2100 which Ive used for 5+ years for podcasting and YouTube BUT this thing is amazing for my current use case. Im using it as a boom above my head for YouTube videos. Wow! At about 2-3 feet away it is perfect. Very rich sound for my bassy voice. I recommend this to anyone who wants a budget mic with awesome sound.\n",
            "\n",
            "After Lowercasing:\n",
            " oh my word i am sold i love my at2100 which ive used for 5+ years for podcasting and youtube but this thing is amazing for my current use case. im using it as a boom above my head for youtube videos. wow! at about 2-3 feet away it is perfect. very rich sound for my bassy voice. i recommend this to anyone who wants a budget mic with awesome sound.\n",
            "\n",
            "After Tokenization:\n",
            " ['oh', 'my', 'word', 'i', 'am', 'sold', 'i', 'love', 'my', 'at2100', 'which', 'ive', 'used', 'for', '5+', 'years', 'for', 'podcasting', 'and', 'youtube', 'but', 'this', 'thing', 'is', 'amazing', 'for', 'my', 'current', 'use', 'case', '.', 'im', 'using', 'it', 'as', 'a', 'boom', 'above', 'my', 'head', 'for', 'youtube', 'videos', '.', 'wow', '!', 'at', 'about', '2-3', 'feet', 'away', 'it', 'is', 'perfect', '.', 'very', 'rich', 'sound', 'for', 'my', 'bassy', 'voice', '.', 'i', 'recommend', 'this', 'to', 'anyone', 'who', 'wants', 'a', 'budget', 'mic', 'with', 'awesome', 'sound', '.']\n",
            "\n",
            "After Removing Stopwords:\n",
            " ['oh', 'word', 'sold', 'love', 'at2100', 'ive', 'used', '5+', 'years', 'podcasting', 'youtube', 'thing', 'amazing', 'current', 'use', 'case', '.', 'im', 'using', 'boom', 'head', 'youtube', 'videos', '.', 'wow', '!', '2-3', 'feet', 'away', 'perfect', '.', 'rich', 'sound', 'bassy', 'voice', '.', 'recommend', 'anyone', 'wants', 'budget', 'mic', 'awesome', 'sound', '.']\n",
            "\n",
            "After Removing Punctuation:\n",
            " ['oh', 'word', 'sold', 'love', 'ive', 'used', 'years', 'podcasting', 'youtube', 'thing', 'amazing', 'current', 'use', 'case', 'im', 'using', 'boom', 'head', 'youtube', 'videos', 'wow', 'feet', 'away', 'perfect', 'rich', 'sound', 'bassy', 'voice', 'recommend', 'anyone', 'wants', 'budget', 'mic', 'awesome', 'sound']\n",
            "\n",
            "Preprocessed text saved: /content/drive/My Drive/IR_Assignment1/text_files/preprocessed_files/file851_preprocessed.txt\n",
            "\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text)\n",
        "        return [word for word in tokens if word.isalpha() and word not in self.stop_words]\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self, directory_path):\n",
        "        self.directory_path = directory_path\n",
        "        self.index = defaultdict(set)\n",
        "\n",
        "    def build_index(self):\n",
        "        preprocessor = TextPreprocessor()\n",
        "        for file_name in os.listdir(self.directory_path):\n",
        "            file_path = os.path.join(self.directory_path, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                tokens = preprocessor.preprocess(text)\n",
        "                for token in tokens:\n",
        "                    self.index[token].add(os.path.basename(file_path))\n",
        "\n",
        "    def save_to_file(self, file_name):\n",
        "        with open(file_name, 'wb') as file:\n",
        "            pickle.dump(self.index, file)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_from_file(file_name):\n",
        "        with open(file_name, 'rb') as file:\n",
        "            return pickle.load(file)\n",
        "\n",
        "class QueryProcessor:\n",
        "    def __init__(self, inverted_index):\n",
        "        self.inverted_index = inverted_index\n",
        "\n",
        "    def process_query(self, query, operations):\n",
        "        preprocessor = TextPreprocessor()\n",
        "        query_terms = preprocessor.preprocess(query)\n",
        "        if not query_terms:\n",
        "            return set()\n",
        "\n",
        "        result_set = self.inverted_index.get(query_terms[0], set())\n",
        "        for i, operation in enumerate(operations):\n",
        "            if i + 1 < len(query_terms):\n",
        "                next_term_set = self.inverted_index.get(query_terms[i + 1], set())\n",
        "                result_set = self.perform_operation(result_set, next_term_set, operation)\n",
        "        return result_set\n",
        "\n",
        "    @staticmethod\n",
        "    def perform_operation(doc_set1, doc_set2, operation):\n",
        "        if operation == 'AND':\n",
        "            return doc_set1 & doc_set2\n",
        "        elif operation == 'OR':\n",
        "            return doc_set1 | doc_set2\n",
        "        elif operation == 'AND NOT':\n",
        "            return doc_set1 - doc_set2\n",
        "        return doc_set1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory_path = '/content/drive/My Drive/IR_Assignment1/text_files/preprocessed_files'\n",
        "    index = InvertedIndex(directory_path)\n",
        "    index.build_index()\n",
        "    index.save_to_file('inverted_index.pkl')\n",
        "\n",
        "    loaded_index = InvertedIndex.load_from_file('inverted_index.pkl')\n",
        "    processor = QueryProcessor(loaded_index)\n",
        "\n",
        "    number_of_queries = int(input(\"Enter the number of queries: \"))\n",
        "    for i in range(number_of_queries):\n",
        "        query = input(f\"Enter query {i+1}: \")\n",
        "        operations = input(f\"Enter operations for query {i+1} separated by comma: \").split(', ')\n",
        "        result_set = processor.process_query(query, operations)\n",
        "\n",
        "        print(f\"Query {i+1}: {query}\")\n",
        "        print(f\"Documents retrieved: {len(result_set)}\")\n",
        "        print(f\"Document names: {', '.join(sorted(result_set))}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnmYvVw1Knqo",
        "outputId": "dae3665f-ea88-4fd3-ea76-0443505e6b8c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 2\n",
            "Enter query 1: Car bag in a canister\n",
            "Enter operations for query 1 separated by comma: OR, AND NOT\n",
            "Query 1: Car bag in a canister\n",
            "Documents retrieved: 31\n",
            "Document names: file118_preprocessed.txt, file166_preprocessed.txt, file174_preprocessed.txt, file264_preprocessed.txt, file313_preprocessed.txt, file363_preprocessed.txt, file3_preprocessed.txt, file404_preprocessed.txt, file459_preprocessed.txt, file466_preprocessed.txt, file542_preprocessed.txt, file573_preprocessed.txt, file665_preprocessed.txt, file682_preprocessed.txt, file686_preprocessed.txt, file698_preprocessed.txt, file699_preprocessed.txt, file738_preprocessed.txt, file73_preprocessed.txt, file746_preprocessed.txt, file780_preprocessed.txt, file797_preprocessed.txt, file860_preprocessed.txt, file863_preprocessed.txt, file864_preprocessed.txt, file886_preprocessed.txt, file892_preprocessed.txt, file930_preprocessed.txt, file942_preprocessed.txt, file956_preprocessed.txt, file981_preprocessed.txt\n",
            "\n",
            "Enter query 2: Coffee brewing techniques in cookbook\n",
            "Enter operations for query 2 separated by comma: AND, OR NOT, OR\n",
            "Query 2: Coffee brewing techniques in cookbook\n",
            "Documents retrieved: 0\n",
            "Document names: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text)\n",
        "        return [word for word in tokens if word.isalpha() and word not in self.stop_words]\n",
        "\n",
        "def defaultdict_list():\n",
        "    \"\"\"Return a defaultdict with list as default factory.\"\"\"\n",
        "    return defaultdict(list)\n",
        "\n",
        "def create_positional_index(directory_path, processor):\n",
        "    positional_index = defaultdict(defaultdict_list)\n",
        "    for file_name in filter(lambda name: name.endswith(\".txt\"), os.listdir(directory_path)):\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            tokens = processor.preprocess(text)\n",
        "            for position, token in enumerate(tokens):\n",
        "                positional_index[token][file_name].append(position)\n",
        "    return positional_index\n",
        "\n",
        "def save_object(obj, file_name):\n",
        "    with open(file_name, 'wb') as file:\n",
        "        pickle.dump(obj, file)\n",
        "\n",
        "def load_object(file_name):\n",
        "    with open(file_name, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "def process_phrase_query(query, positional_index, processor):\n",
        "    tokens = processor.preprocess(query)\n",
        "    if not tokens:\n",
        "        return 0, []\n",
        "\n",
        "    common_docs = set.intersection(*(set(positional_index[token].keys()) for token in tokens))\n",
        "    valid_docs = []\n",
        "    for doc in common_docs:\n",
        "        positions = [positional_index[token][doc] for token in tokens]\n",
        "        if any(all(p2 - p1 == 1 for p1, p2 in zip(pos_list, pos_list[1:])) for pos_list in zip(*positions)):\n",
        "            valid_docs.append(doc)\n",
        "\n",
        "    return len(valid_docs), valid_docs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory_path = '/content/drive/My Drive/IR_Assignment1/text_files/preprocessed_files'\n",
        "    processor = TextProcessor()\n",
        "    positional_index = create_positional_index(directory_path, processor)\n",
        "    index_file_name = 'positional_index.pkl'\n",
        "    save_object(positional_index, index_file_name)\n",
        "\n",
        "    loaded_positional_index = load_object(index_file_name)\n",
        "\n",
        "    number_of_queries = int(input(\"Enter the number of queries: \"))\n",
        "    queries = [input(f\"Enter query {i+1}: \") for i in range(number_of_queries)]\n",
        "\n",
        "    for i, query in enumerate(queries):\n",
        "        count, documents = process_phrase_query(query, loaded_positional_index, processor)\n",
        "        print(f\"Number of documents retrieved for query {i+1} using positional index: {count}\")\n",
        "        print(f\"Names of documents retrieved for query {i+1} using positional index: {', '.join(documents)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq52ULt5QWtf",
        "outputId": "6c1934df-9fd0-463b-dfd7-a229ab95147b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 3\n",
            "Enter query 1: it is a good in front for poutch\n",
            "Enter query 2: it is good in reliable for fit\n",
            "Enter query 3: it is a fit front poutch\n",
            "Number of documents retrieved for query 1 using positional index: 0\n",
            "Names of documents retrieved for query 1 using positional index: \n",
            "\n",
            "Number of documents retrieved for query 2 using positional index: 1\n",
            "Names of documents retrieved for query 2 using positional index: file9_preprocessed.txt\n",
            "\n",
            "Number of documents retrieved for query 3 using positional index: 1\n",
            "Names of documents retrieved for query 3 using positional index: file9_preprocessed.txt\n",
            "\n"
          ]
        }
      ]
    }
  ]
}